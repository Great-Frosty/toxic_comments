{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-07T15:43:23.630822Z","iopub.execute_input":"2021-11-07T15:43:23.631112Z","iopub.status.idle":"2021-11-07T15:43:31.104657Z","shell.execute_reply.started":"2021-11-07T15:43:23.631063Z","shell.execute_reply":"2021-11-07T15:43:31.103751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport copy\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:31.111249Z","iopub.execute_input":"2021-11-07T15:43:31.111426Z","iopub.status.idle":"2021-11-07T15:43:31.423419Z","shell.execute_reply.started":"2021-11-07T15:43:31.1114Z","shell.execute_reply":"2021-11-07T15:43:31.422582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/toxic-comments-classification-apdl-2021/train_data.csv')\ntest_df = pd.read_csv('../input/toxic-comments-classification-apdl-2021/test_data.csv')\nprint(df.columns)\nprint(df.shape)\ntarget_col= df.columns[1]\nfeature_col= df.columns[0]\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:31.425092Z","iopub.execute_input":"2021-11-07T15:43:31.425617Z","iopub.status.idle":"2021-11-07T15:43:31.542236Z","shell.execute_reply.started":"2021-11-07T15:43:31.42557Z","shell.execute_reply":"2021-11-07T15:43:31.541419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom transformers import (AutoTokenizer, AutoModel, \n                          AutoModelForSequenceClassification, \n                          DataCollatorWithPadding, AdamW, get_scheduler,\n                          get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n                          )\n\nimport pyarrow as pa\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport datasets\nimport random\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:31.544995Z","iopub.execute_input":"2021-11-07T15:43:31.545229Z","iopub.status.idle":"2021-11-07T15:43:34.166713Z","shell.execute_reply.started":"2021-11-07T15:43:31.545184Z","shell.execute_reply":"2021-11-07T15:43:34.165852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_value = 42\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:34.172365Z","iopub.execute_input":"2021-11-07T15:43:34.172792Z","iopub.status.idle":"2021-11-07T15:43:34.183612Z","shell.execute_reply.started":"2021-11-07T15:43:34.172708Z","shell.execute_reply":"2021-11-07T15:43:34.179757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.15, random_state=seed_value)\nprint(len(train_df))\nprint(len(val_df))\nprint(len(test_df))","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:34.185621Z","iopub.execute_input":"2021-11-07T15:43:34.1864Z","iopub.status.idle":"2021-11-07T15:43:34.202006Z","shell.execute_reply.started":"2021-11-07T15:43:34.186359Z","shell.execute_reply":"2021-11-07T15:43:34.201328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.reset_index(inplace=True)\ntrain_df.drop(\"index\", axis=1, inplace=True)\n\nval_df.reset_index(inplace=True)\nval_df.drop(\"index\", axis=1, inplace=True)\n\ntest_df.reset_index(inplace=True)\ntest_df.drop(\"index\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:34.203397Z","iopub.execute_input":"2021-11-07T15:43:34.203907Z","iopub.status.idle":"2021-11-07T15:43:34.226579Z","shell.execute_reply.started":"2021-11-07T15:43:34.20387Z","shell.execute_reply":"2021-11-07T15:43:34.22589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"DeepPavlov/distilrubert-base-cased-conversational\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:34.227918Z","iopub.execute_input":"2021-11-07T15:43:34.228395Z","iopub.status.idle":"2021-11-07T15:43:35.016456Z","shell.execute_reply.started":"2021-11-07T15:43:34.228359Z","shell.execute_reply":"2021-11-07T15:43:35.015681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:35.018023Z","iopub.execute_input":"2021-11-07T15:43:35.018284Z","iopub.status.idle":"2021-11-07T15:43:35.023395Z","shell.execute_reply.started":"2021-11-07T15:43:35.018247Z","shell.execute_reply":"2021-11-07T15:43:35.022572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens = tokenizer.batch_encode_plus(train_df[\"comment\"].tolist(),\n                                           max_length = 200,\n                                           padding=True,\n                                           truncation=True,\n                                           return_token_type_ids=False\n                                           )\n\nval_tokens = tokenizer.batch_encode_plus(val_df[\"comment\"].tolist(),\n                                         max_length = 200,\n                                         padding=True,\n                                         truncation=True,\n                                         return_token_type_ids=False\n                                         )\n\ntest_tokens = tokenizer.batch_encode_plus(test_df[\"comment\"].tolist(),\n                                          max_length = 200,\n                                          padding=True,\n                                          truncation=True,\n                                          return_token_type_ids=False\n                                          )","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:35.025316Z","iopub.execute_input":"2021-11-07T15:43:35.025655Z","iopub.status.idle":"2021-11-07T15:43:37.31241Z","shell.execute_reply.started":"2021-11-07T15:43:35.025619Z","shell.execute_reply":"2021-11-07T15:43:37.311593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\ntrain_y = torch.tensor(train_df.toxic.to_numpy()).unsqueeze(-1)\n\nval_seq = torch.tensor(val_tokens['input_ids'])\nval_mask = torch.tensor(val_tokens['attention_mask'])\nval_y = torch.tensor(val_df.toxic.to_numpy()).unsqueeze(-1)\n\ntest_seq = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:37.313691Z","iopub.execute_input":"2021-11-07T15:43:37.313979Z","iopub.status.idle":"2021-11-07T15:43:37.989759Z","shell.execute_reply.started":"2021-11-07T15:43:37.313944Z","shell.execute_reply":"2021-11-07T15:43:37.988895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\n\nval_data = TensorDataset(val_seq, val_mask, val_y)\nval_sampler = SequentialSampler(val_data)\n\ntest_data = TensorDataset(test_seq, test_mask)\ntest_sampler = SequentialSampler(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:37.991241Z","iopub.execute_input":"2021-11-07T15:43:37.99163Z","iopub.status.idle":"2021-11-07T15:43:37.997417Z","shell.execute_reply.started":"2021-11-07T15:43:37.991588Z","shell.execute_reply":"2021-11-07T15:43:37.996315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 220\n\ntrain_loader = DataLoader(train_data, \n                              sampler=train_sampler, \n                              batch_size=batch_size,\n#                               collate_fn=data_collator\n                              )\n\nval_loader = DataLoader(val_data, \n                            sampler = val_sampler, \n                            batch_size=batch_size,\n#                             collate_fn=data_collator\n                            )\ntest_loader = DataLoader(test_data,\n                            sampler = test_sampler,\n                            batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:38.001912Z","iopub.execute_input":"2021-11-07T15:43:38.002215Z","iopub.status.idle":"2021-11-07T15:43:38.009595Z","shell.execute_reply.started":"2021-11-07T15:43:38.002176Z","shell.execute_reply":"2021-11-07T15:43:38.008858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deleteEncodingLayers(model, num_layers_to_keep): \n    old_module_list = model.distilbert.transformer.layer\n    new_module_list = nn.ModuleList()\n\n    for i in range(0, num_layers_to_keep):\n        new_module_list.append(old_module_list[i])\n\n    model_copy = copy.deepcopy(model)\n    model_copy.distilbert.transformer.layer = new_module_list\n\n    return model_copy","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:38.012731Z","iopub.execute_input":"2021-11-07T15:43:38.012997Z","iopub.status.idle":"2021-11-07T15:43:38.021471Z","shell.execute_reply.started":"2021-11-07T15:43:38.012971Z","shell.execute_reply":"2021-11-07T15:43:38.020686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 1)\nmodel.dropout = nn.Dropout(p=0.1, inplace=False)\nmodel = deleteEncodingLayers(model, 3)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:38.022412Z","iopub.execute_input":"2021-11-07T15:43:38.022617Z","iopub.status.idle":"2021-11-07T15:43:41.612783Z","shell.execute_reply.started":"2021-11-07T15:43:38.022594Z","shell.execute_reply":"2021-11-07T15:43:41.611978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device);","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:41.614285Z","iopub.execute_input":"2021-11-07T15:43:41.614541Z","iopub.status.idle":"2021-11-07T15:43:43.958816Z","shell.execute_reply.started":"2021-11-07T15:43:41.614504Z","shell.execute_reply":"2021-11-07T15:43:43.957956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARN_RATE = 3e-5\noptimizer = AdamW(model.parameters(),\n                  lr = LEARN_RATE, \n                  eps = 1e-8 \n                  )\nepochs = 2\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps\n                                            )\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:43.960191Z","iopub.execute_input":"2021-11-07T15:43:43.960667Z","iopub.status.idle":"2021-11-07T15:43:43.972028Z","shell.execute_reply.started":"2021-11-07T15:43:43.960624Z","shell.execute_reply":"2021-11-07T15:43:43.970609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(output, target):\n    output = torch.sigmoid(output) >= 0.5\n    target = target == 1.0\n    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:43.974992Z","iopub.execute_input":"2021-11-07T15:43:43.977663Z","iopub.status.idle":"2021-11-07T15:43:43.985187Z","shell.execute_reply.started":"2021-11-07T15:43:43.977623Z","shell.execute_reply":"2021-11-07T15:43:43.984551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:43.987296Z","iopub.execute_input":"2021-11-07T15:43:43.98801Z","iopub.status.idle":"2021-11-07T15:43:44.005962Z","shell.execute_reply.started":"2021-11-07T15:43:43.98797Z","shell.execute_reply":"2021-11-07T15:43:44.003178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, model, criterion, optimizer, epoch):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for batch in stream:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        output = model(b_input_ids, \n                       attention_mask=b_input_mask, \n                       labels=b_labels).logits\n        loss = criterion(output, b_labels)\n        accuracy = calculate_accuracy(output, b_labels)\n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", accuracy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n        )","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:44.006982Z","iopub.execute_input":"2021-11-07T15:43:44.007387Z","iopub.status.idle":"2021-11-07T15:43:44.022334Z","shell.execute_reply.started":"2021-11-07T15:43:44.007352Z","shell.execute_reply":"2021-11-07T15:43:44.021501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, model, criterion, epoch):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for batch in stream:\n                b_input_ids = batch[0].to(device)\n                b_input_mask = batch[1].to(device)\n                b_labels = batch[2].to(device)\n                output = model(b_input_ids, \n                               attention_mask=b_input_mask, \n                               labels=b_labels).logits\n            \n                loss = criterion(output, b_labels)\n                accuracy = calculate_accuracy(output, b_labels)\n\n                metric_monitor.update(\"Loss\", loss.item())\n                metric_monitor.update(\"Accuracy\", accuracy)\n                stream.set_description(\n                    \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n                )","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:44.023583Z","iopub.execute_input":"2021-11-07T15:43:44.024263Z","iopub.status.idle":"2021-11-07T15:43:44.034742Z","shell.execute_reply.started":"2021-11-07T15:43:44.024212Z","shell.execute_reply":"2021-11-07T15:43:44.034126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, epochs + 1):\n    train(train_loader, model, criterion, optimizer, epoch)\n    validate(val_loader, model, criterion, epoch)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:43:44.035632Z","iopub.execute_input":"2021-11-07T15:43:44.036Z","iopub.status.idle":"2021-11-07T15:45:11.648488Z","shell.execute_reply.started":"2021-11-07T15:43:44.035965Z","shell.execute_reply":"2021-11-07T15:45:11.647497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(test_loader, model):\n    model.eval()\n    stream = tqdm(test_loader)\n    \n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(stream)) :\n                b_input_ids = batch[0].to(device)\n#                 print(b_input_ids)\n                b_input_mask = batch[1].to(device)\n                output = model(b_input_ids, \n                               attention_mask=b_input_mask).logits\n                pred_probs = (output>0.5).int()\n                pred_probas_sigma = torch.sigmoid(output)\n                if step == 0:\n                    predictions = pred_probs.cpu().detach().numpy()\n                    pred_probas = pred_probas_sigma.cpu().detach().numpy()\n                else:\n                    predictions = np.append(predictions, pred_probs.cpu().detach().numpy(), axis=0)\n                    pred_probas = np.append(pred_probas, pred_probas_sigma.cpu().detach().numpy(), axis=0)\n                \n    return predictions, pred_probas\n                \n                \nresults, res_probas = predict(test_loader, model)\n\npredictions_df = pd.DataFrame(results, columns = ['toxic'])\nsubmission = pd.concat([test_df[\"comment_id\"], predictions_df], axis=1)\nsubmission.to_csv('submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:45:11.650183Z","iopub.execute_input":"2021-11-07T15:45:11.650482Z","iopub.status.idle":"2021-11-07T15:45:17.118901Z","shell.execute_reply.started":"2021-11-07T15:45:11.650441Z","shell.execute_reply":"2021-11-07T15:45:17.118192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_results, res_probas = predict(val_loader, model)\ncomparison = torch.eq(torch.Tensor(val_results), val_y)\ndiff = (comparison == 0).nonzero(as_tuple=True)[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:45:17.120164Z","iopub.execute_input":"2021-11-07T15:45:17.12057Z","iopub.status.idle":"2021-11-07T15:45:19.614145Z","shell.execute_reply.started":"2021-11-07T15:45:17.120506Z","shell.execute_reply":"2021-11-07T15:45:19.613387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deleteEncodingLayers2(model, num_layers_to_keep): \n    old_module_list = model.bert.encoder.layer\n    new_module_list = nn.ModuleList()\n\n    for i in range(0, num_layers_to_keep):\n        new_module_list.append(old_module_list[i])\n\n    model_copy = copy.deepcopy(model)\n    model_copy.bert.encoder.layer = new_module_list\n\n    return model_copy","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:45:19.615733Z","iopub.execute_input":"2021-11-07T15:45:19.616244Z","iopub.status.idle":"2021-11-07T15:45:19.622628Z","shell.execute_reply.started":"2021-11-07T15:45:19.6162Z","shell.execute_reply":"2021-11-07T15:45:19.621853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint2 = \"DeepPavlov/rubert-base-cased\"\ntokenizer2 = AutoTokenizer.from_pretrained(checkpoint2)\n\ntrain_tokens = tokenizer2.batch_encode_plus(train_df[\"comment\"].tolist(),\n                                           max_length = 200,\n                                           padding=True,\n                                           truncation=True,\n                                           return_token_type_ids=False\n                                           )\n\nval_tokens = tokenizer2.batch_encode_plus(val_df[\"comment\"].tolist(),\n                                         max_length = 200,\n                                         padding=True,\n                                         truncation=True,\n                                         return_token_type_ids=False\n                                         )\n\ntest_tokens = tokenizer2.batch_encode_plus(test_df[\"comment\"].tolist(),\n                                          max_length = 200,\n                                          padding=True,\n                                          truncation=True,\n                                          return_token_type_ids=False\n                                          )\n\ntrain_seq = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\ntrain_y = torch.tensor(train_df.toxic.to_numpy()).unsqueeze(-1)\n\nval_seq = torch.tensor(val_tokens['input_ids'])\nval_mask = torch.tensor(val_tokens['attention_mask'])\nval_y = torch.tensor(val_df.toxic.to_numpy()).unsqueeze(-1)\n\ntest_seq = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\n\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\n\nval_data = TensorDataset(val_seq, val_mask, val_y)\nval_sampler = SequentialSampler(val_data)\n\ntest_data = TensorDataset(test_seq, test_mask)\ntest_sampler = SequentialSampler(test_data)\n\nbatch_size = 220\n\ntrain_loader = DataLoader(train_data,\n                              sampler=train_sampler, \n                              batch_size=batch_size,\n#                               collate_fn=data_collator\n                              )\n\nval_loader = DataLoader(val_data,\n                            sampler = val_sampler, \n                            batch_size=batch_size,\n#                             collate_fn=data_collator\n                            )\ntest_loader = DataLoader(test_data,\n                            sampler = test_sampler,\n                            batch_size=batch_size)\n\nmodel2 = AutoModelForSequenceClassification.from_pretrained(checkpoint2, num_labels = 1)\n# model2.dropout = nn.Dropout(p=0.1, inplace=False)\nmodel2 = deleteEncodingLayers2(model2, 2)\n\nmodel2.to(device);\n\noptimizer2 = AdamW(model2.parameters(),\n                  lr = LEARN_RATE, \n                  eps = 1e-8 \n                  )\nepochs = 4\ntotal_steps = len(train_loader) * epochs\nscheduler2 = get_linear_schedule_with_warmup(optimizer2, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps\n                                            )\ncriterion2 = nn.BCEWithLogitsLoss()\n\nfor epoch in range(1, epochs + 1):\n    train(train_loader, model2, criterion2, optimizer2, epoch)\n    validate(val_loader, model2, criterion2, epoch)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:45:19.624221Z","iopub.execute_input":"2021-11-07T15:45:19.624739Z","iopub.status.idle":"2021-11-07T15:47:27.58334Z","shell.execute_reply.started":"2021-11-07T15:45:19.624699Z","shell.execute_reply":"2021-11-07T15:47:27.58259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_results2, res_probas2 = predict(val_loader, model2)\ncomparison2 = torch.eq(torch.Tensor(val_results2), val_y)\ndiff2 = (comparison2 == 0).nonzero(as_tuple=True)[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:47:27.584786Z","iopub.execute_input":"2021-11-07T15:47:27.585611Z","iopub.status.idle":"2021-11-07T15:47:29.322524Z","shell.execute_reply.started":"2021-11-07T15:47:27.585566Z","shell.execute_reply":"2021-11-07T15:47:29.321734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens = np.concatenate((res_probas, res_probas2), axis=1)\nalpha = 0.8\nresults = np.average(ens, axis=1, weights=[alpha,1 - alpha])\nresults_sigma = (results>=0.5)\n\ncomparison3 = torch.eq(torch.Tensor(results_sigma).int().unsqueeze(-1), val_y)\n\ndiff3 = (comparison3 == 0).nonzero(as_tuple=True)[0]\ndiff = (comparison == 0).nonzero(as_tuple=True)[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:50:05.133396Z","iopub.execute_input":"2021-11-07T15:50:05.133935Z","iopub.status.idle":"2021-11-07T15:50:05.141387Z","shell.execute_reply.started":"2021-11-07T15:50:05.133897Z","shell.execute_reply":"2021-11-07T15:50:05.140617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = val_y == 1.0\ntorch.true_divide((target == torch.Tensor(results_sigma).unsqueeze(-1)).sum(dim=0), results_sigma.shape[0]).item()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T15:50:07.166804Z","iopub.execute_input":"2021-11-07T15:50:07.167243Z","iopub.status.idle":"2021-11-07T15:50:07.174128Z","shell.execute_reply.started":"2021-11-07T15:50:07.167208Z","shell.execute_reply":"2021-11-07T15:50:07.173091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}